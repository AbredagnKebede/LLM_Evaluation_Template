I need you to run the tests in @temp_tests.py which holds unit tests which are testing the code inside @temp_code.py which is a code generated by a prompt in @prompt.md. 

First go to @temp_tests.py and change relevant functions which you must use from @temp_code.py. The content of @temp_code.py changes thus the function names also change. Update the relevant parts of @temp_tests.py so that it can test @temp_code.py properly. 

Then run temp_test.py, look into the logs of the failling tests and the errors happened. Once the errors/failing tests are gathered, then I need you to judge specific metrics about the code. Those metrics includes Accuracy, Instruction Following, Efficiency & Optimality and Other Issues.


1. Accuracy: Does the AI's response correctly and completely address the information and code requirements?

Core areas we should look for in this case:

> Factual correctness
> Comprehensive Answer (No missing key points)
> Code syntax errors
> Code functional errors

Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

You will check the accuracy of the generated response then only choose one of the four. (No issues, Minor issues, Moderate issues, Major issues). Then generate the reason to briefly explain why a particular rating (Major, Moderate, Minor) was given. 

For example, the reason can be something like: 
- "The model incorrectly stated that JSON is not supported in Python." OR
- "Accurate and aligns with the user’s request."


2. Instruction Following: Is the provided response on-point & respects all constraints in the user prompt? Is it tailored for the User skill level?

Core areas we should look for in this case:

> Comprehends and adheres to all constraints and requests of user
> Addresses all the requests of the user (Exceptions will be requests that are outside the capability of the LLM. For example: Give me a sorting algorithm with O(log(n)) time OR Give me the production ready React app to track student data.)
> Focus remains on the user’s request
> Not too short to skip the important and helpful information
> Not too verbose to include unnecessary details
> Well-tailored for the skill level of the user

Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

You will only choose one of the four. (No issues, Minor issues, Moderate issues, Major issues). Then generate the reason to briefly explain why a particular rating (Major, Moderate, Minor) was given. 

For example, the reason can be something like: 
- "Response missed key edge case around null input handling." OR
- "Fully addressed all points from the prompt."

3.Efficiency & Optimality: Is the AI's response optimal in terms of the approach, code complexity, case coverage, and the method suggested in response to the user's prompt?

Core areas we should look for in this case:

> Optimality in terms of Time and Memory complexity (It is fine if assistant gives an algorithm which is efficient and used in mainstream rather than a complex algorithm which optimizes on time/memory a little bit more.)
> Handles all the edge cases
> Takes care of security aspect of code
> During Q&A, suggest optimal answer to the user


Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

You will only choose one of the four. (No issues, Minor issues, Moderate issues, Major issues). 

Then generate the reason to briefly explain why a particular rating (Major, Moderate, Minor) was given. 

4. Other Issues: Are there any other significant issues in the response that affect user experience but were not covered in the predefined categories?

We want you to not be confined by predefined categories while accessing the quality of response. For example: 1) Assistant forgets the context of its previous conversation. 2) The React Component provided by the assistant has very bad user experience. 3) Become overly apologetic about the things that can’t be done by the assistant.


Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

You will only choose one of the four. (No issues, Minor issues, Moderate issues, Major issues). 

Then generate the reason to briefly explain why a particular rating (Major, Moderate, Minor) was given. 

For example, the comment can be something like: 
- "Tone was overly casual for a professional setting."
- "Response included an unnecessary disclaimer that distracted from the answer."

5. Final Score: Rate the score as a preference for the model as a rating.
  5- Exemplary
  4- Good
  3- Fair
  2- Bad
  1- Terrible


An example of an what an output looks like:

Output: 

```
Accuracy: Major Issues
- The code imports `numpy` instead of using the required `os` library, which violates the mandatory constraint for system-level file operations.
- The implementation uses basic Python lists rather than cache-efficient data structures that are explicitly required by the prompt.
- The solution cannot handle datasets exceeding 10^6 elements properly because it loads entire datasets into memory instead of processing them in fixed-size blocks.
- The algorithm uses linear searches with O(n) complexity instead of the required O(log n) operations, making it unsuitable for large-scale processing.
- The ant colony optimization logic is too simple and lacks proper pheromone trail management and sophisticated heuristic calculations.
- The output format is incorrect because the code returns actual data values instead of optimized order indices as specified in the requirements.
- The probability calculations in path selection are flawed, and the pheromone update mechanisms don't reflect solution quality effectively.

Instruction Following: Major Issues
- The response ignores the explicit requirement to use the `os` library for file operations, which represents a fundamental constraint violation.
- The code doesn't address scale limitations for datasets larger than 10^6 elements and fails to process data in fixed-size blocks as required.
- The implementation lacks cache-optimization strategies entirely and provides no mechanisms to minimize cache misses or enhance data retrieval speed.
- The solution is missing the required testing framework component from objective seven, leaving the implementation incomplete and unverifiable.
- The code doesn't create a simulation environment for ant behavior modeling and lacks dynamic pheromone adjustment mechanisms based on solution quality.
- The algorithm doesn't achieve the required O(log n) time complexity for selection steps and instead uses basic linear operations.
- The implementation doesn't demonstrate comprehension of the sports analytics context or performance constraints outlined in the prompt.

Efficiency & Optimality: Major Issues
- The algorithm has O(n) complexity per selection step instead of the required O(log n), making it completely unsuitable for large-scale data processing.
- For datasets with 10^9 elements, the implementation would consume excessive memory and processing time, violating the sub-quadratic performance constraint.
- The code has no cache optimization strategies and misses opportunities to improve data locality and reduce memory access latency.
- The solution lacks edge case handlers for empty datasets and duplicate values, making it unable to handle boundary conditions in real scenarios.
- The pheromone update mechanism is overly simple and doesn't balance exploration versus exploitation trade-offs properly.
- The heuristic calculation uses simple reciprocals, which is mathematically incorrect for sorting applications and doesn't provide meaningful guidance.
- The absence of file-based processing makes the solution inadequate for the specified scale requirements and performance constraints.

Other Issues: Moderate Issues
- The unauthorized `numpy` import creates an external dependency that could cause failures in constrained environments where only standard libraries are available.
- The code structure lacks modularity and extensibility, making it difficult to adapt for different sports analytics scenarios or dynamic parameter modifications.
- The implementation has no error handling throughout the codebase, which would cause crashes when encountering unexpected input conditions in production.
- The code doesn't follow proper software engineering practices and lacks documentation, logging, monitoring capabilities, and proper organization standards.
- The variable naming conventions are inconsistent and don't follow Python best practices for readable and maintainable code.
- The code organization doesn't reflect the complexity of the cache-optimized ACO algorithm described in the detailed prompt specifications.
- The solution shows poor understanding of memory usage and processing speed trade-offs that are fundamental in high-performance analytics environments.

Final Score: 1 - Terrible
```
Inaddtion to the above output also I want you to:
- Insert each justification(Accuracy, Efficiency & Optimality, Instruction Following, Other Issues) as a comment in the correct place where it happend in @temp_code.py.
- If the justification refers to a specific line, place it above that line.
- If it refers to a whole function, place it immediately above the function definition.
- If it refers to the whole file, place it at the top of the file.
- After adding the justification comment to the @temp_code.py without change the code, it will look something like:
```python
# Accuracy:
# - The code imports `numpy` instead of using the required `os` library, which
#   violates the mandatory constraint for system-level file operations.
#
# Instruction Following:
# - The response ignores the explicit requirement to use the `os` library for
#   file operations, which represents a fundamental constraint violation.
import os
import random
import numpy as np

def initialize_pheromones(length: int) -> list[float]:
    """Initialize pheromones with a small positive value."""
    return [0.1] * length

# Efficiency & Optimality:
# - The heuristic calculation uses simple reciprocals, which is mathematically
#   incorrect for sorting applications and doesn't provide meaningful guidance.
#
# Accuracy:
# - The ant colony optimization logic is too simple and lacks sophisticated
def compute_heuristic(data: list[int]) -> list[float]:
    """Compute heuristic values as 1/value for simplicity."""
    return [1.0 / (val if val != 0 else 1) for val in data]

# Efficiency & Optimality:
# - The algorithm has O(n) complexity per selection step instead of the required

```

Notes:
 - Generate the output within plain text block ```
 - @tests.py is generated to import functions from ideal_code.py. But I want you to prepare a temp_tests.py file which have the exact same unit tests as @tests.py but that will be run against the code I will provide below. Make sure the unit tests are exaxtly the same.
 - We aren't allowed to edit temp_code.py. If the function names didn't match with the temp_tests.py, change the function names in the temp_tests.py just for the sake of continuing running the tests and see what failed. But mention this failure in the final outputs in the relevant metrics.
 - The bullet points in the output, they should be only relavent to the title. For example under accurary, just list things concerted with accuracy issues only. 
 - Avoid reduduncy. For example in Instructuon Following, each bullet point should be unique.
 - Avoid overexplanation and verbose comments. Make each bullet point as consice as possible but at the same time don't miss any information.
 - Make it sound as natural as possible. Each bullet point must sound as if a real human wrote them with unique naunces of a human. Don't use any non-ascii characters. 
 - Each section(Accuracy, Instruction Following, etc) descriptions must have between 150 to 200 words for each(total word count of all bulletpoints). 
 - Refine the justifications until there are no errors in grammer at all. Think of yourself as a **Content Specialist** with expertise in American English conventions. Review the provided text (the "description") and create a single Markdown table summarizing feedback in the following six categories. Then if there are suggestion, correct the content and reassess it again.
| Category              | Details                                                     |
|-----------------------|-------------------------------------------------------------|
| Grammar               | Subject-verb agreement, tense, spelling, etc.               |
| Punctuation           | Commas, periods, semicolons, consistency.                   |
| Article Usage         | Correct use of "a," "an," "the."                            |
| Style Consistency     | Uniform tone, tense, formality.                             |
| Sentence Structure    | Clarity, readability, complexity.                           |
| Final Suggested Version | The corrected, ideal sentence or text.                    |

prompt.md: 
temp_code.py:
temp_tests.py: 